{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 128, 128, 3)\n",
      "128 128\n",
      "128 128\n"
     ]
    }
   ],
   "source": [
    "#conv Neural Network\n",
    "# tensorboard --logdir=/home/ncc/notebook/learn/tensorboard/log\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "test_img=np.load(file_locate+'test_img.npy');\n",
    "print np.shape(test_img)\n",
    "img_row = np.shape(test_img)[1]\n",
    "img_col = np.shape(test_img)[2]\n",
    "\n",
    "batch_size=30\n",
    "print img_row ,img_col\n",
    "n_classes =2\n",
    "in_ch =3\n",
    "out_ch1=512\n",
    "out_ch2=512\n",
    "out_ch3=512\n",
    "out_ch4=512\n",
    "out_ch5=512\n",
    "out_ch6=512\n",
    "out_ch7=512\n",
    "out_ch8=512\n",
    "out_ch9=512\n",
    "out_ch10=512\n",
    "out_ch11=512\n",
    "out_ch12=512\n",
    "out_ch13=512\n",
    "fully_ch1=4096\n",
    "fully_ch2 =4096\n",
    "\n",
    "x= tf.placeholder(\"float\",shape=[None,img_col , img_row , 3],  name = 'x-input')\n",
    "y_=tf.placeholder(\"float\",shape=[None , n_classes] , name = 'y-input')\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "x_image= tf.reshape(x,[-1,img_row,img_col,3])\n",
    "\n",
    "iterate=300000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weight_row =3 ; weight_col=3\n",
    "\n",
    "pooling_row_size1=int(img_row/2)\n",
    "pooling_row_size2=int(pooling_row_size1/2)\n",
    "pooling_row_size3=int(pooling_row_size2/2)\n",
    "pooling_row_size4=int(pooling_row_size3/2)\n",
    "pooling_row_size5=int(pooling_row_size4/2)\n",
    "pooling_col_size1=int(img_col/2)\n",
    "pooling_col_size2=int(pooling_col_size1/2)\n",
    "pooling_col_size3=int(pooling_col_size2/2)\n",
    "pooling_col_size4=int(pooling_col_size3/2)\n",
    "pooling_col_size5=int(pooling_col_size4/2)\n",
    "\n",
    "print img_col , img_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (12825, 128, 128, 3)\n",
      "Training Data Label (12825, 2)\n",
      "Test Data Label (1604, 2)\n",
      "val Data Label (1603, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "    #with tf.device('/gpu:1'):\n",
    "    train_img=np.load(file_locate+'train_img.npy');\n",
    "    train_lab=np.load(file_locate+'train_lab.npy');\n",
    "    val_img= np.load(file_locate+'val_img.npy');\n",
    "    val_lab = np.load(file_locate+'val_lab.npy');\n",
    "    test_img=np.load(file_locate+'test_img.npy');\n",
    "    test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "    print \"Training Data\",np.shape(train_img)\n",
    "    print \"Training Data Label\",np.shape(train_lab)\n",
    "    print \"Test Data Label\",np.shape(test_lab)\n",
    "    print \"val Data Label\" , np.shape(val_img)\n",
    "\n",
    "    n_train= np.shape(train_img)[0]\n",
    "    n_train_lab = np.shape(train_lab)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"def weight_variable(name,shape):\n",
    "    #initial = tf.truncated_normal(shape , stddev=0.1)\n",
    "    initial = tf.get_variable(name,shape=shape , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return tf.Variable(initial)\"\"\"\n",
    "with tf.device('/gpu:2'):\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1 , shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "\n",
    "    def conv2d(x,w):\n",
    "        return tf.nn.conv2d(x,w, strides = [1,1,1,1], padding='SAME')\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x , ksize=[1,2,2,1] ,strides = [1,2,2,1] , padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    \n",
    "    w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,3,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv1 = bias_variable([out_ch1])\n",
    "\n",
    "    w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv2= bias_variable([out_ch2])\n",
    "\n",
    "    w_conv3 = tf.get_variable(\"W3\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv3 = bias_variable([out_ch3])\n",
    "\n",
    "    w_conv4 =tf.get_variable(\"W4\" ,[weight_row,weight_col,out_ch3,out_ch4] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv4 = bias_variable([out_ch4])\n",
    "with tf.device('/gpu:3'):\n",
    "    w_conv5 = tf.get_variable(\"W5\",[weight_row,weight_col,out_ch4,out_ch5] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv5 = bias_variable([out_ch5])\n",
    "    \n",
    "    w_conv6= tf.get_variable(\"W6\" , [weight_row , weight_col ,out_ch5, out_ch6] ,initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv6 = bias_variable([out_ch6])\n",
    "\n",
    "    w_conv7 = tf.get_variable(\"W7\" , [weight_row , weight_col ,out_ch6 , out_ch7 ], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv7 = bias_variable([out_ch7])\n",
    "    \n",
    "    w_conv8 = tf.get_variable(\"W8\" , [weight_row , weight_col ,out_ch7 , out_ch8 ] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv8 = bias_variable([out_ch8])\n",
    "    \n",
    "    w_conv9 = tf.get_variable(\"W9\" ,[weight_row , weight_col ,out_ch8 , out_ch9] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv9 = bias_variable([out_ch9])\n",
    "with tf.device('/gpu:2'):\n",
    "    w_conv10 = tf.get_variable(\"W10\" ,[weight_row ,weight_col ,out_ch9  ,out_ch10] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv10 = bias_variable([out_ch10])\n",
    "    \n",
    "    w_conv11 = tf.get_variable(\"W11\" ,[weight_row , weight_col ,out_ch10 , out_ch11 ] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv11 = bias_variable([out_ch11])\n",
    "                               \n",
    "    w_conv12 = tf.get_variable(\"W12\" , [ weight_row , weight_col , out_ch11 , out_ch12 ] ,initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv12 =bias_variable([out_ch12])\n",
    "                               \n",
    "    w_conv13 = tf.get_variable(\"W13\" , [weight_row , weight_col ,out_ch12 , out_ch13] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv13 = bias_variable([out_ch13])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_13:0\", shape=(?, 128, 128, 512), dtype=float32, device=/device:GPU:3)\n",
      "Tensor(\"Relu:0\", shape=(?, 128, 128, 512), dtype=float32, device=/device:GPU:2)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 128, 128, 512), dtype=float32, device=/device:GPU:2)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 64, 64, 512), dtype=float32, device=/device:GPU:2)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 64, 64, 512), dtype=float32, device=/device:GPU:2)\n",
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "#conncect hidden layer \n",
    "with tf.device('/gpu:2'):\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image , w_conv1)+b_conv1)\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1 , w_conv2)+b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2 , w_conv3)+b_conv3)\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3 , w_conv4)+b_conv4)\n",
    "    h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "    h_conv5 = tf.nn.relu(conv2d(h_pool4, w_conv5)+b_conv5)\n",
    "    h_conv6= tf.nn.relu(conv2d(h_conv5 , w_conv6)+b_conv6)\n",
    "    h_conv7= tf.nn.relu(conv2d(h_conv6 , w_conv7)+ b_conv7)\n",
    "    h_pool7 = max_pool_2x2(h_conv7)\n",
    "with tf.device('/gpu:3'):\n",
    "    h_conv8 = tf.nn.relu(conv2d(h_pool7 , w_conv8)+b_conv8)              \n",
    "    h_conv9= tf.nn.relu(conv2d(h_conv8 , w_conv9)+b_conv9)\n",
    "    h_conv10= tf.nn.relu(conv2d(h_conv9 , w_conv10)+b_conv10)\n",
    "    h_pool10 = max_pool_2x2(h_conv10)\n",
    "    \n",
    "    h_conv11 = tf.nn.relu(conv2d(h_pool10 , w_conv11)+b_conv11)\n",
    "    h_conv12 = tf.nn.relu(conv2d(h_conv11 , w_conv12)+b_conv12)              \n",
    "    h_conv13= tf.nn.relu(conv2d(h_conv12 , w_conv13)+b_conv13)\n",
    "    h_pool13 = max_pool_2x2(h_conv13)\n",
    "                         \n",
    "                    \n",
    "                         \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    h_pool5= max_pool_2x2(h_conv5)\n",
    "\n",
    "    print conv2d(x_image , w_conv1)+b_conv1\n",
    "    print h_conv1\n",
    "    \n",
    "    print h_conv2\n",
    "    print h_conv3\n",
    "    print h_conv4\n",
    "\n",
    "    print b_conv2.get_shape()\n",
    "    \n",
    "    #print conv2d(h_pool1 , w_conv2).get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 512)\n"
     ]
    }
   ],
   "source": [
    "print w_conv1.get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pooling_col_size4=int(h_pool13.get_shape()[2])\n",
    "pooling_row_size4=int(h_pool13.get_shape()[1])\n",
    "#connect fully connected layer \n",
    "with tf.device('/gpu:2'):\n",
    "    w_fc1=tf.get_variable(\"fc1\",[pooling_col_size4*pooling_row_size4*out_ch13,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_fc1 = bias_variable([fully_ch1])\n",
    "\n",
    "    h_pool5_flat =tf.reshape(h_pool13, [-1,pooling_col_size4*pooling_row_size4*out_ch13])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool5_flat , w_fc1)+ b_fc1)\n",
    "with tf.device('/gpu:3'):\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    w_fc2 =tf.get_variable(\"fc2\",[fully_ch1 , n_classes],initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_fc2 = bias_variable([n_classes])\n",
    "\n",
    "    y_conv=tf.add(tf.matmul(h_fc1_drop,w_fc2),b_fc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is recorded at :45\n"
     ]
    }
   ],
   "source": [
    "#dirname = '/home/ncc/notebook/mammo/result/'\n",
    "\n",
    "dirname='/mnt/Jupyter/seongjung_mnt/Eye/result/'\n",
    "\n",
    "count=0\n",
    "while(True):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    elif not os.path.isdir(dirname + str(count)):\n",
    "        dirname=dirname+str(count)\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    else:\n",
    "        count+=1\n",
    "print 'it is recorded at :'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open(dirname+\"/log.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 , training  accuracy 0.48718\n",
      "step 0 , loss : 0.993452\n",
      "step 100 , training  accuracy 0.48718\n",
      "step 100 , loss : 0.704402\n",
      "step 200 , training  accuracy 0.512821\n",
      "step 200 , loss : 0.693702\n",
      "step 300 , training  accuracy 0.512821\n",
      "step 300 , loss : 0.692826\n",
      "step 400 , training  accuracy 0.48718\n",
      "step 400 , loss : 0.70108\n",
      "step 500 , training  accuracy 0.520655\n",
      "step 500 , loss : 0.692991\n",
      "step 600 , training  accuracy 0.512821\n",
      "step 600 , loss : 0.693282\n",
      "step 700 , training  accuracy 0.48718\n",
      "step 700 , loss : 0.693257\n",
      "step 800 , training  accuracy 0.512821\n",
      "step 800 , loss : 0.694775\n",
      "step 900 , training  accuracy 0.48718\n",
      "step 900 , loss : 0.699284\n",
      "step 1000 , training  accuracy 0.48718\n",
      "step 1000 , loss : 0.6936\n",
      "step 1100 , training  accuracy 0.48718\n",
      "step 1100 , loss : 0.693191\n",
      "step 1200 , training  accuracy 0.512821\n",
      "step 1200 , loss : 0.693033\n",
      "step 1300 , training  accuracy 0.48718\n",
      "step 1300 , loss : 0.693524\n",
      "step 1400 , training  accuracy 0.48718\n",
      "step 1400 , loss : 0.693848\n",
      "step 1500 , training  accuracy 0.512821\n",
      "step 1500 , loss : 0.693146\n",
      "step 1600 , training  accuracy 0.512821\n",
      "step 1600 , loss : 0.69305\n",
      "step 1700 , training  accuracy 0.48718\n",
      "step 1700 , loss : 0.707992\n",
      "step 1800 , training  accuracy 0.48718\n",
      "step 1800 , loss : 0.693567\n",
      "step 1900 , training  accuracy 0.48718\n",
      "step 1900 , loss : 0.693424\n",
      "step 2000 , training  accuracy 0.512821\n",
      "step 2000 , loss : 0.693109\n",
      "step 2100 , training  accuracy 0.48718\n",
      "step 2100 , loss : 0.693279\n",
      "step 2200 , training  accuracy 0.512821\n",
      "step 2200 , loss : 0.694863\n",
      "step 2300 , training  accuracy 0.512821\n",
      "step 2300 , loss : 0.69294\n",
      "step 2400 , training  accuracy 0.512821\n",
      "step 2400 , loss : 0.694553\n",
      "step 2500 , training  accuracy 0.48718\n",
      "step 2500 , loss : 0.694112\n",
      "step 2600 , training  accuracy 0.48718\n",
      "step 2600 , loss : 0.696751\n",
      "step 2700 , training  accuracy 0.512821\n",
      "step 2700 , loss : 0.693396\n",
      "step 2800 , training  accuracy 0.527493\n",
      "step 2800 , loss : 0.691571\n",
      "step 2900 , training  accuracy 0.48718\n",
      "step 2900 , loss : 0.696251\n",
      "step 3000 , training  accuracy 0.48718\n",
      "step 3000 , loss : 0.694087\n",
      "step 3100 , training  accuracy 0.48718\n",
      "step 3100 , loss : 0.694308\n",
      "step 3200 , training  accuracy 0.48718\n",
      "step 3200 , loss : 0.694004\n",
      "step 3300 , training  accuracy 0.48718\n",
      "step 3300 , loss : 0.693748\n",
      "step 3400 , training  accuracy 0.512821\n",
      "step 3400 , loss : 0.693717\n",
      "step 3500 , training  accuracy 0.48718\n",
      "step 3500 , loss : 0.693605\n",
      "step 3600 , training  accuracy 0.48718\n",
      "step 3600 , loss : 0.69768\n",
      "step 3700 , training  accuracy 0.48718\n",
      "step 3700 , loss : 0.694786\n",
      "step 3800 , training  accuracy 0.48718\n",
      "step 3800 , loss : 0.694515\n",
      "step 3900 , training  accuracy 0.512821\n",
      "step 3900 , loss : 0.693631\n",
      "step 4000 , training  accuracy 0.512821\n",
      "step 4000 , loss : 0.693043\n",
      "step 4100 , training  accuracy 0.48718\n",
      "step 4100 , loss : 0.69632\n",
      "step 4200 , training  accuracy 0.512821\n",
      "step 4200 , loss : 0.69308\n",
      "step 4300 , training  accuracy 0.48718\n",
      "step 4300 , loss : 0.695842\n",
      "step 4400 , training  accuracy 0.48718\n",
      "step 4400 , loss : 0.696047\n",
      "step 4500 , training  accuracy 0.512821\n",
      "step 4500 , loss : 0.693229\n",
      "step 4600 , training  accuracy 0.48718\n",
      "step 4600 , loss : 0.693177\n",
      "step 4700 , training  accuracy 0.512821\n",
      "step 4700 , loss : 0.693971\n",
      "step 4800 , training  accuracy 0.512821\n",
      "step 4800 , loss : 0.693234\n",
      "step 4900 , training  accuracy 0.48718\n",
      "step 4900 , loss : 0.700896\n",
      "step 5000 , training  accuracy 0.48718\n",
      "step 5000 , loss : 0.700022\n",
      "step 5100 , training  accuracy 0.48718\n",
      "step 5100 , loss : 0.698453\n",
      "step 5200 , training  accuracy 0.48718\n",
      "step 5200 , loss : 0.693437\n",
      "step 5300 , training  accuracy 0.48718\n",
      "step 5300 , loss : 0.693437\n",
      "step 5400 , training  accuracy 0.48718\n",
      "step 5400 , loss : 0.69414\n",
      "step 5500 , training  accuracy 0.512821\n",
      "step 5500 , loss : 0.692799\n",
      "step 5600 , training  accuracy 0.48718\n",
      "step 5600 , loss : 0.69338\n",
      "step 5700 , training  accuracy 0.48718\n",
      "step 5700 , loss : 0.695277\n",
      "step 5800 , training  accuracy 0.48718\n",
      "step 5800 , loss : 0.693305\n",
      "step 5900 , training  accuracy 0.512821\n",
      "step 5900 , loss : 0.693739\n",
      "step 6000 , training  accuracy 0.48718\n",
      "step 6000 , loss : 0.694616\n",
      "step 6100 , training  accuracy 0.512821\n",
      "step 6100 , loss : 0.693014\n",
      "step 6200 , training  accuracy 0.48718\n",
      "step 6200 , loss : 0.693289\n",
      "step 6300 , training  accuracy 0.48718\n",
      "step 6300 , loss : 0.694307\n",
      "step 6400 , training  accuracy 0.48718\n",
      "step 6400 , loss : 0.695161\n",
      "step 6500 , training  accuracy 0.48718\n",
      "step 6500 , loss : 0.702316\n",
      "step 6600 , training  accuracy 0.512821\n",
      "step 6600 , loss : 0.69377\n",
      "step 6700 , training  accuracy 0.48718\n",
      "step 6700 , loss : 0.694353\n",
      "step 6800 , training  accuracy 0.512821\n",
      "step 6800 , loss : 0.693071\n",
      "step 6900 , training  accuracy 0.48718\n",
      "step 6900 , loss : 0.699665\n",
      "step 7000 , training  accuracy 0.512821\n",
      "step 7000 , loss : 0.693126\n",
      "step 7100 , training  accuracy 0.48718\n",
      "step 7100 , loss : 0.695758\n",
      "step 7200 , training  accuracy 0.512821\n",
      "step 7200 , loss : 0.702898\n",
      "step 7300 , training  accuracy 0.512821\n",
      "step 7300 , loss : 0.693542\n",
      "step 7400 , training  accuracy 0.48718\n",
      "step 7400 , loss : 0.693201\n",
      "step 7500 , training  accuracy 0.512821\n",
      "step 7500 , loss : 0.693191\n",
      "step 7600 , training  accuracy 0.512821\n",
      "step 7600 , loss : 0.693028\n",
      "step 7700 , training  accuracy 0.48718\n",
      "step 7700 , loss : 0.695235\n",
      "step 7800 , training  accuracy 0.512821\n",
      "step 7800 , loss : 0.693313\n",
      "step 7900 , training  accuracy 0.48718\n",
      "step 7900 , loss : 0.693297\n",
      "step 8000 , training  accuracy 0.48718\n",
      "step 8000 , loss : 0.694938\n",
      "step 8100 , training  accuracy 0.48718\n",
      "step 8100 , loss : 0.696235\n",
      "step 8200 , training  accuracy 0.48718\n",
      "step 8200 , loss : 0.694515\n",
      "step 8300 , training  accuracy 0.512821\n",
      "step 8300 , loss : 0.693309\n",
      "step 8400 , training  accuracy 0.512821\n",
      "step 8400 , loss : 0.694986\n",
      "step 8500 , training  accuracy 0.512821\n",
      "step 8500 , loss : 0.698285\n",
      "step 8600 , training  accuracy 0.48718\n",
      "step 8600 , loss : 0.693898\n",
      "step 8700 , training  accuracy 0.48718\n",
      "step 8700 , loss : 0.693724\n",
      "step 8800 , training  accuracy 0.48718\n",
      "step 8800 , loss : 0.693568\n",
      "step 8900 , training  accuracy 0.512821\n",
      "step 8900 , loss : 0.693\n",
      "step 9000 , training  accuracy 0.48718\n",
      "step 9000 , loss : 0.695712\n",
      "step 9100 , training  accuracy 0.48718\n",
      "step 9100 , loss : 0.694324\n",
      "step 9200 , training  accuracy 0.512821\n",
      "step 9200 , loss : 0.69301\n",
      "step 9300 , training  accuracy 0.48718\n",
      "step 9300 , loss : 0.694249\n",
      "step 9400 , training  accuracy 0.48718\n",
      "step 9400 , loss : 0.693378\n",
      "step 9500 , training  accuracy 0.48718\n",
      "step 9500 , loss : 0.69606\n",
      "step 9600 , training  accuracy 0.512821\n",
      "step 9600 , loss : 0.695087\n",
      "step 9700 , training  accuracy 0.512821\n",
      "step 9700 , loss : 0.693001\n",
      "step 9800 , training  accuracy 0.512821\n",
      "step 9800 , loss : 0.693409\n",
      "step 9900 , training  accuracy 0.512821\n",
      "step 9900 , loss : 0.693068\n",
      "step 10000 , training  accuracy 0.512821\n",
      "step 10000 , loss : 0.693017\n",
      "step 10100 , training  accuracy 0.512821\n",
      "step 10100 , loss : 0.693252\n",
      "step 10200 , training  accuracy 0.512821\n",
      "step 10200 , loss : 0.693135\n",
      "step 10300 , training  accuracy 0.48718\n",
      "step 10300 , loss : 0.693159\n",
      "step 10400 , training  accuracy 0.48718\n",
      "step 10400 , loss : 0.693594\n",
      "step 10500 , training  accuracy 0.512821\n",
      "step 10500 , loss : 0.693959\n",
      "step 10600 , training  accuracy 0.48718\n",
      "step 10600 , loss : 0.696343\n",
      "step 10700 , training  accuracy 0.48718\n",
      "step 10700 , loss : 0.696632\n",
      "step 10800 , training  accuracy 0.48718\n",
      "step 10800 , loss : 0.693548\n",
      "step 10900 , training  accuracy 0.48718\n",
      "step 10900 , loss : 0.694039\n",
      "step 11000 , training  accuracy 0.48718\n",
      "step 11000 , loss : 0.695633\n",
      "step 11100 , training  accuracy 0.512821\n",
      "step 11100 , loss : 0.693547\n",
      "step 11200 , training  accuracy 0.512821\n",
      "step 11200 , loss : 0.696325\n",
      "step 11300 , training  accuracy 0.512821\n",
      "step 11300 , loss : 0.693141\n",
      "step 11400 , training  accuracy 0.48718\n",
      "step 11400 , loss : 0.695294\n",
      "step 11500 , training  accuracy 0.512821\n",
      "step 11500 , loss : 0.694028\n",
      "step 11600 , training  accuracy 0.512821\n",
      "step 11600 , loss : 0.693001\n",
      "step 11700 , training  accuracy 0.48718\n",
      "step 11700 , loss : 0.696948\n",
      "step 11800 , training  accuracy 0.48718\n",
      "step 11800 , loss : 0.693393\n",
      "step 11900 , training  accuracy 0.48718\n",
      "step 11900 , loss : 0.693793\n",
      "step 12000 , training  accuracy 0.48718\n",
      "step 12000 , loss : 0.693878\n",
      "step 12100 , training  accuracy 0.48718\n",
      "step 12100 , loss : 0.693712\n",
      "step 12200 , training  accuracy 0.48718\n",
      "step 12200 , loss : 0.69528\n",
      "step 12300 , training  accuracy 0.48718\n",
      "step 12300 , loss : 0.693932\n",
      "step 12400 , training  accuracy 0.512821\n",
      "step 12400 , loss : 0.693074\n",
      "step 12500 , training  accuracy 0.48718\n",
      "step 12500 , loss : 0.693957\n",
      "step 12600 , training  accuracy 0.48718\n",
      "step 12600 , loss : 0.693437\n",
      "step 12700 , training  accuracy 0.48718\n",
      "step 12700 , loss : 0.693745\n",
      "step 12800 , training  accuracy 0.48718\n",
      "step 12800 , loss : 0.695037\n",
      "step 12900 , training  accuracy 0.512821\n",
      "step 12900 , loss : 0.693044\n",
      "step 13000 , training  accuracy 0.512821\n",
      "step 13000 , loss : 0.692999\n",
      "step 13100 , training  accuracy 0.512821\n",
      "step 13100 , loss : 0.693498\n",
      "step 13200 , training  accuracy 0.48718\n",
      "step 13200 , loss : 0.69351\n",
      "step 13300 , training  accuracy 0.48718\n",
      "step 13300 , loss : 0.693735\n",
      "step 13400 , training  accuracy 0.48718\n",
      "step 13400 , loss : 0.693179\n",
      "step 13500 , training  accuracy 0.48718\n",
      "step 13500 , loss : 0.693459\n",
      "step 13600 , training  accuracy 0.48718\n",
      "step 13600 , loss : 0.693672\n",
      "step 13700 , training  accuracy 0.512821\n",
      "step 13700 , loss : 0.692997\n",
      "step 13800 , training  accuracy 0.512821\n",
      "step 13800 , loss : 0.693\n",
      "step 13900 , training  accuracy 0.512821\n",
      "step 13900 , loss : 0.693382\n",
      "step 14000 , training  accuracy 0.48718\n",
      "step 14000 , loss : 0.693741\n",
      "step 14100 , training  accuracy 0.512821\n",
      "step 14100 , loss : 0.693075\n",
      "step 14200 , training  accuracy 0.512821\n",
      "step 14200 , loss : 0.693012\n",
      "step 14300 , training  accuracy 0.512821\n",
      "step 14300 , loss : 0.692988\n",
      "step 14400 , training  accuracy 0.48718\n",
      "step 14400 , loss : 0.693655\n",
      "step 14500 , training  accuracy 0.512821\n",
      "step 14500 , loss : 0.693559\n",
      "step 14600 , training  accuracy 0.48718\n",
      "step 14600 , loss : 0.695132\n",
      "step 14700 , training  accuracy 0.512821\n",
      "step 14700 , loss : 0.693124\n",
      "step 14800 , training  accuracy 0.512821\n",
      "step 14800 , loss : 0.693051\n",
      "step 14900 , training  accuracy 0.512821\n",
      "step 14900 , loss : 0.693098\n",
      "step 15000 , training  accuracy 0.48718\n",
      "step 15000 , loss : 0.69393\n",
      "step 15100 , training  accuracy 0.48718\n",
      "step 15100 , loss : 0.693385\n",
      "step 15200 , training  accuracy 0.512821\n",
      "step 15200 , loss : 0.693159\n",
      "step 15300 , training  accuracy 0.512821\n",
      "step 15300 , loss : 0.693215\n",
      "step 15400 , training  accuracy 0.512821\n",
      "step 15400 , loss : 0.693002\n",
      "step 15500 , training  accuracy 0.512821\n",
      "step 15500 , loss : 0.693895\n",
      "step 15600 , training  accuracy 0.48718\n",
      "step 15600 , loss : 0.693159\n",
      "step 15700 , training  accuracy 0.48718\n",
      "step 15700 , loss : 0.693644\n",
      "step 15800 , training  accuracy 0.512821\n",
      "step 15800 , loss : 0.693229\n",
      "step 15900 , training  accuracy 0.512821\n",
      "step 15900 , loss : 0.693001\n",
      "step 16000 , training  accuracy 0.48718\n",
      "step 16000 , loss : 0.69339\n",
      "step 16100 , training  accuracy 0.512821\n",
      "step 16100 , loss : 0.693012\n",
      "step 16200 , training  accuracy 0.48718\n",
      "step 16200 , loss : 0.693817\n",
      "step 16300 , training  accuracy 0.512821\n",
      "step 16300 , loss : 0.693061\n",
      "step 16400 , training  accuracy 0.512821\n",
      "step 16400 , loss : 0.693066\n",
      "step 16500 , training  accuracy 0.512821\n",
      "step 16500 , loss : 0.693054\n",
      "step 16600 , training  accuracy 0.48718\n",
      "step 16600 , loss : 0.693654\n",
      "step 16700 , training  accuracy 0.48718\n",
      "step 16700 , loss : 0.693212\n",
      "step 16800 , training  accuracy 0.512821\n",
      "step 16800 , loss : 0.693003\n",
      "step 16900 , training  accuracy 0.512203\n",
      "step 16900 , loss : 0.693147\n",
      "step 17000 , training  accuracy 0.48718\n",
      "step 17000 , loss : 0.693664\n",
      "step 17100 , training  accuracy 0.512821\n",
      "step 17100 , loss : 0.693006\n",
      "step 17200 , training  accuracy 0.512821\n",
      "step 17200 , loss : 0.693027\n",
      "step 17300 , training  accuracy 0.48718\n",
      "step 17300 , loss : 0.693231\n",
      "step 17400 , training  accuracy 0.48718\n",
      "step 17400 , loss : 0.693415\n",
      "step 17500 , training  accuracy 0.48718\n",
      "step 17500 , loss : 0.693202\n",
      "step 17600 , training  accuracy 0.48718\n",
      "step 17600 , loss : 0.693283\n",
      "step 17700 , training  accuracy 0.512821\n",
      "step 17700 , loss : 0.693058\n",
      "step 17800 , training  accuracy 0.48718\n",
      "step 17800 , loss : 0.693422\n",
      "step 17900 , training  accuracy 0.48718\n",
      "step 17900 , loss : 0.693203\n",
      "step 18000 , training  accuracy 0.48718\n",
      "step 18000 , loss : 0.694127\n",
      "step 18100 , training  accuracy 0.48718\n",
      "step 18100 , loss : 0.694138\n",
      "step 18200 , training  accuracy 0.48718\n",
      "step 18200 , loss : 0.693186\n",
      "step 18300 , training  accuracy 0.512821\n",
      "step 18300 , loss : 0.693047\n",
      "step 18400 , training  accuracy 0.512821\n",
      "step 18400 , loss : 0.693\n",
      "step 18500 , training  accuracy 0.512821\n",
      "step 18500 , loss : 0.693263\n",
      "step 18600 , training  accuracy 0.48718\n",
      "step 18600 , loss : 0.693173\n",
      "step 18700 , training  accuracy 0.512821\n",
      "step 18700 , loss : 0.693107\n",
      "step 18800 , training  accuracy 0.512821\n",
      "step 18800 , loss : 0.693001\n",
      "step 18900 , training  accuracy 0.48718\n",
      "step 18900 , loss : 0.694133\n",
      "step 19000 , training  accuracy 0.48718\n",
      "step 19000 , loss : 0.694627\n",
      "step 19100 , training  accuracy 0.48718\n",
      "step 19100 , loss : 0.693405\n",
      "step 19200 , training  accuracy 0.512821\n",
      "step 19200 , loss : 0.693026\n",
      "step 19300 , training  accuracy 0.48718\n",
      "step 19300 , loss : 0.694497\n",
      "step 19400 , training  accuracy 0.48718\n",
      "step 19400 , loss : 0.693566\n",
      "step 19500 , training  accuracy 0.48718\n",
      "step 19500 , loss : 0.693283\n",
      "step 19600 , training  accuracy 0.512821\n",
      "step 19600 , loss : 0.693105\n",
      "step 19700 , training  accuracy 0.48718\n",
      "step 19700 , loss : 0.693503\n",
      "step 19800 , training  accuracy 0.48718\n",
      "step 19800 , loss : 0.693885\n",
      "step 19900 , training  accuracy 0.512821\n",
      "step 19900 , loss : 0.692998\n",
      "step 20000 , training  accuracy 0.512821\n",
      "step 20000 , loss : 0.693017\n",
      "step 20100 , training  accuracy 0.48718\n",
      "step 20100 , loss : 0.694559\n",
      "step 20200 , training  accuracy 0.48718\n",
      "step 20200 , loss : 0.693388\n",
      "step 20300 , training  accuracy 0.512821\n",
      "step 20300 , loss : 0.693163\n",
      "step 20400 , training  accuracy 0.48718\n",
      "step 20400 , loss : 0.696337\n",
      "step 20500 , training  accuracy 0.48718\n",
      "step 20500 , loss : 0.693566\n",
      "step 20600 , training  accuracy 0.48718\n",
      "step 20600 , loss : 0.693224\n",
      "step 20700 , training  accuracy 0.512821\n",
      "step 20700 , loss : 0.69303\n",
      "step 20800 , training  accuracy 0.48718\n",
      "step 20800 , loss : 0.69334\n",
      "step 20900 , training  accuracy 0.512821\n",
      "step 20900 , loss : 0.693414\n",
      "step 21000 , training  accuracy 0.48718\n",
      "step 21000 , loss : 0.695616\n",
      "step 21100 , training  accuracy 0.512821\n",
      "step 21100 , loss : 0.693463\n",
      "step 21200 , training  accuracy 0.48718\n",
      "step 21200 , loss : 0.69316\n",
      "step 21300 , training  accuracy 0.512821\n",
      "step 21300 , loss : 0.693125\n",
      "step 21400 , training  accuracy 0.48718\n",
      "step 21400 , loss : 0.693829\n",
      "step 21500 , training  accuracy 0.512821\n",
      "step 21500 , loss : 0.693068\n",
      "step 21600 , training  accuracy 0.48718\n",
      "step 21600 , loss : 0.693188\n",
      "step 21700 , training  accuracy 0.512821\n",
      "step 21700 , loss : 0.693049\n",
      "step 21800 , training  accuracy 0.48718\n",
      "step 21800 , loss : 0.693962\n",
      "step 21900 , training  accuracy 0.512821\n",
      "step 21900 , loss : 0.693269\n",
      "step 22000 , training  accuracy 0.48718\n",
      "step 22000 , loss : 0.693408\n",
      "step 22100 , training  accuracy 0.512821\n",
      "step 22100 , loss : 0.693001\n",
      "step 22200 , training  accuracy 0.512821\n",
      "step 22200 , loss : 0.693018\n",
      "step 22300 , training  accuracy 0.48718\n",
      "step 22300 , loss : 0.694565\n",
      "step 22400 , training  accuracy 0.48718\n",
      "step 22400 , loss : 0.693281\n",
      "step 22500 , training  accuracy 0.512821\n",
      "step 22500 , loss : 0.693033\n",
      "step 22600 , training  accuracy 0.48718\n",
      "step 22600 , loss : 0.693272\n",
      "step 22700 , training  accuracy 0.48718\n",
      "step 22700 , loss : 0.693267\n",
      "step 22800 , training  accuracy 0.48718\n",
      "step 22800 , loss : 0.693399\n",
      "step 22900 , training  accuracy 0.48718\n",
      "step 22900 , loss : 0.693375\n",
      "step 23000 , training  accuracy 0.512821\n",
      "step 23000 , loss : 0.693109\n",
      "step 23100 , training  accuracy 0.512821\n",
      "step 23100 , loss : 0.693164\n",
      "step 23200 , training  accuracy 0.512821\n",
      "step 23200 , loss : 0.692998\n",
      "step 23300 , training  accuracy 0.512821\n",
      "step 23300 , loss : 0.693021\n",
      "step 23400 , training  accuracy 0.512821\n",
      "step 23400 , loss : 0.693061\n",
      "step 23500 , training  accuracy 0.512821\n",
      "step 23500 , loss : 0.69313\n",
      "step 23600 , training  accuracy 0.512821\n",
      "step 23600 , loss : 0.693138\n",
      "step 23700 , training  accuracy 0.512821\n",
      "step 23700 , loss : 0.693089\n",
      "step 23800 , training  accuracy 0.512821\n",
      "step 23800 , loss : 0.693056\n",
      "step 23900 , training  accuracy 0.48718\n",
      "step 23900 , loss : 0.693263\n",
      "step 24000 , training  accuracy 0.512821\n",
      "step 24000 , loss : 0.693109\n",
      "step 24100 , training  accuracy 0.48718\n",
      "step 24100 , loss : 0.693211\n",
      "step 24200 , training  accuracy 0.512821\n",
      "step 24200 , loss : 0.693036\n",
      "step 24300 , training  accuracy 0.512821\n",
      "step 24300 , loss : 0.693145\n",
      "step 24400 , training  accuracy 0.512821\n",
      "step 24400 , loss : 0.693002\n",
      "step 24500 , training  accuracy 0.512821\n",
      "step 24500 , loss : 0.693012\n",
      "step 24600 , training  accuracy 0.48718\n",
      "step 24600 , loss : 0.693801\n",
      "step 24700 , training  accuracy 0.512821\n",
      "step 24700 , loss : 0.693109\n",
      "step 24800 , training  accuracy 0.48718\n",
      "step 24800 , loss : 0.693284\n",
      "step 24900 , training  accuracy 0.48718\n",
      "step 24900 , loss : 0.693501\n",
      "step 25000 , training  accuracy 0.48718\n",
      "step 25000 , loss : 0.693712\n",
      "step 25100 , training  accuracy 0.512821\n",
      "step 25100 , loss : 0.693012\n",
      "step 25200 , training  accuracy 0.48718\n",
      "step 25200 , loss : 0.693639\n",
      "step 25300 , training  accuracy 0.512821\n",
      "step 25300 , loss : 0.693006\n",
      "step 25400 , training  accuracy 0.48718\n",
      "step 25400 , loss : 0.693206\n",
      "step 25500 , training  accuracy 0.512821\n",
      "step 25500 , loss : 0.693002\n",
      "step 25600 , training  accuracy 0.512821\n",
      "step 25600 , loss : 0.693093\n",
      "step 25700 , training  accuracy 0.512821\n",
      "step 25700 , loss : 0.693018\n",
      "step 25800 , training  accuracy 0.48718\n",
      "step 25800 , loss : 0.693821\n",
      "step 25900 , training  accuracy 0.512821\n",
      "step 25900 , loss : 0.693131\n",
      "step 26000 , training  accuracy 0.48718\n",
      "step 26000 , loss : 0.693545\n",
      "step 26100 , training  accuracy 0.512821\n",
      "step 26100 , loss : 0.693072\n",
      "step 26200 , training  accuracy 0.512821\n",
      "step 26200 , loss : 0.693048\n",
      "step 26300 , training  accuracy 0.48718\n",
      "step 26300 , loss : 0.693207\n",
      "step 26400 , training  accuracy 0.512821\n",
      "step 26400 , loss : 0.693036\n",
      "step 26500 , training  accuracy 0.48718\n",
      "step 26500 , loss : 0.693881\n",
      "step 26600 , training  accuracy 0.512821\n",
      "step 26600 , loss : 0.692999\n",
      "step 26700 , training  accuracy 0.48718\n",
      "step 26700 , loss : 0.69504\n",
      "step 26800 , training  accuracy 0.48718\n",
      "step 26800 , loss : 0.693193\n",
      "step 26900 , training  accuracy 0.48718\n",
      "step 26900 , loss : 0.693584\n",
      "step 27000 , training  accuracy 0.512821\n",
      "step 27000 , loss : 0.693107\n",
      "step 27100 , training  accuracy 0.48718\n",
      "step 27100 , loss : 0.693241\n",
      "step 27200 , training  accuracy 0.512821\n",
      "step 27200 , loss : 0.693114\n",
      "step 27300 , training  accuracy 0.48718\n",
      "step 27300 , loss : 0.693281\n",
      "step 27400 , training  accuracy 0.48718\n",
      "step 27400 , loss : 0.693642\n",
      "step 27500 , training  accuracy 0.48718\n",
      "step 27500 , loss : 0.704168\n",
      "step 27600 , training  accuracy 0.48718\n",
      "step 27600 , loss : 0.694925\n",
      "step 27700 , training  accuracy 0.550095\n",
      "step 27700 , loss : 0.689061\n",
      "step 27800 , training  accuracy 0.512821\n",
      "step 27800 , loss : 0.693008\n",
      "step 27900 , training  accuracy 0.512251\n",
      "step 27900 , loss : 0.692616\n",
      "step 28000 , training  accuracy 0.48718\n",
      "step 28000 , loss : 0.693559\n",
      "step 28100 , training  accuracy 0.512821\n",
      "step 28100 , loss : 0.69306\n",
      "step 28200 , training  accuracy 0.48718\n",
      "step 28200 , loss : 0.694218\n",
      "step 28300 , training  accuracy 0.491501\n",
      "step 28300 , loss : 0.693261\n",
      "step 28400 , training  accuracy 0.512821\n",
      "step 28400 , loss : 0.693011\n",
      "step 28500 , training  accuracy 0.513438\n",
      "step 28500 , loss : 0.690819\n",
      "step 28600 , training  accuracy 0.512821\n",
      "step 28600 , loss : 0.692499\n",
      "step 28700 , training  accuracy 0.507692\n",
      "step 28700 , loss : 0.693245\n",
      "step 28800 , training  accuracy 0.498291\n",
      "step 28800 , loss : 0.69327\n",
      "step 28900 , training  accuracy 0.550665\n",
      "step 28900 , loss : 0.685808\n",
      "step 29000 , training  accuracy 0.529487\n",
      "step 29000 , loss : 0.688744\n",
      "step 29100 , training  accuracy 0.557692\n",
      "step 29100 , loss : 0.684788\n",
      "step 29200 , training  accuracy 0.569848\n",
      "step 29200 , loss : 0.684774\n",
      "step 29300 , training  accuracy 0.546581\n",
      "step 29300 , loss : 0.686229\n",
      "step 29400 , training  accuracy 0.56491\n",
      "step 29400 , loss : 0.683971\n",
      "step 29500 , training  accuracy 0.566762\n",
      "step 29500 , loss : 0.684546\n",
      "step 29600 , training  accuracy 0.549668\n",
      "step 29600 , loss : 0.68642\n",
      "step 29700 , training  accuracy 0.561396\n",
      "step 29700 , loss : 0.683503\n",
      "step 29800 , training  accuracy 0.560589\n",
      "step 29800 , loss : 0.684627\n",
      "step 29900 , training  accuracy 0.545774\n",
      "step 29900 , loss : 0.688841\n",
      "step 30000 , training  accuracy 0.533048\n",
      "step 30000 , loss : 0.693753\n",
      "step 30100 , training  accuracy 0.57887\n",
      "step 30100 , loss : 0.679923\n",
      "step 30200 , training  accuracy 0.570418\n",
      "step 30200 , loss : 0.681442\n",
      "step 30300 , training  accuracy 0.577208\n",
      "step 30300 , loss : 0.67918\n",
      "step 30400 , training  accuracy 0.575594\n",
      "step 30400 , loss : 0.681498\n",
      "step 30500 , training  accuracy 0.567142\n",
      "step 30500 , loss : 0.680887\n",
      "step 30600 , training  accuracy 0.562868\n",
      "step 30600 , loss : 0.685519\n",
      "step 30700 , training  accuracy 0.572935\n",
      "step 30700 , loss : 0.682054\n",
      "step 30800 , training  accuracy 0.586277\n",
      "step 30800 , loss : 0.678843\n",
      "step 30900 , training  accuracy 0.59226\n",
      "step 30900 , loss : 0.677682\n",
      "step 31000 , training  accuracy 0.584236\n",
      "step 31000 , loss : 0.675753\n",
      "step 31100 , training  accuracy 0.594539\n",
      "step 31100 , loss : 0.676536\n",
      "step 31200 , training  accuracy 0.588557\n",
      "step 31200 , loss : 0.677114\n",
      "step 31300 , training  accuracy 0.596344\n",
      "step 31300 , loss : 0.678944\n",
      "step 31400 , training  accuracy 0.580532\n",
      "step 31400 , loss : 0.678309\n",
      "step 31500 , training  accuracy 0.596391\n",
      "step 31500 , loss : 0.678343\n",
      "step 31600 , training  accuracy 0.601947\n",
      "step 31600 , loss : 0.675904\n",
      "step 31700 , training  accuracy 0.59886\n",
      "step 31700 , loss : 0.673693\n",
      "step 31800 , training  accuracy 0.596581\n",
      "step 31800 , loss : 0.678259\n",
      "step 31900 , training  accuracy 0.605223\n",
      "step 31900 , loss : 0.672644\n",
      "step 32000 , training  accuracy 0.556268\n",
      "step 32000 , loss : 0.695507\n",
      "step 32100 , training  accuracy 0.610351\n",
      "step 32100 , loss : 0.670737\n",
      "step 32200 , training  accuracy 0.632811\n",
      "step 32200 , loss : 0.670635\n",
      "step 32300 , training  accuracy 0.593115\n",
      "step 32300 , loss : 0.677123\n",
      "step 32400 , training  accuracy 0.618614\n",
      "step 32400 , loss : 0.670093\n",
      "step 32500 , training  accuracy 0.619421\n",
      "step 32500 , loss : 0.667464\n",
      "step 32600 , training  accuracy 0.610161\n",
      "step 32600 , loss : 0.671822\n",
      "step 32700 , training  accuracy 0.607075\n",
      "step 32700 , loss : 0.669296\n",
      "step 32800 , training  accuracy 0.632194\n",
      "step 32800 , loss : 0.665146\n",
      "step 32900 , training  accuracy 0.620703\n",
      "step 32900 , loss : 0.66826\n",
      "step 33000 , training  accuracy 0.625594\n",
      "step 33000 , loss : 0.665979\n",
      "step 33100 , training  accuracy 0.62849\n",
      "step 33100 , loss : 0.666479\n",
      "step 33200 , training  accuracy 0.60812\n",
      "step 33200 , loss : 0.67853\n",
      "step 33300 , training  accuracy 0.622745\n",
      "step 33300 , loss : 0.672387\n",
      "step 33400 , training  accuracy 0.633856\n",
      "step 33400 , loss : 0.666462\n",
      "step 33500 , training  accuracy 0.636087\n",
      "step 33500 , loss : 0.665379\n",
      "step 33600 , training  accuracy 0.619801\n",
      "step 33600 , loss : 0.673785\n",
      "step 33700 , training  accuracy 0.625404\n",
      "step 33700 , loss : 0.662607\n",
      "step 33800 , training  accuracy 0.640029\n",
      "step 33800 , loss : 0.662852\n",
      "step 33900 , training  accuracy 0.640598\n",
      "step 33900 , loss : 0.66323\n",
      "step 34000 , training  accuracy 0.655223\n",
      "step 34000 , loss : 0.658473\n",
      "step 34100 , training  accuracy 0.652327\n",
      "step 34100 , loss : 0.660283\n",
      "step 34200 , training  accuracy 0.624786\n",
      "step 34200 , loss : 0.666735\n",
      "step 34300 , training  accuracy 0.643305\n",
      "step 34300 , loss : 0.662668\n",
      "step 34400 , training  accuracy 0.63509\n",
      "step 34400 , loss : 0.666174\n",
      "step 34500 , training  accuracy 0.65603\n",
      "step 34500 , loss : 0.659543\n",
      "step 34600 , training  accuracy 0.645584\n",
      "step 34600 , loss : 0.6662\n",
      "step 34700 , training  accuracy 0.660589\n",
      "step 34700 , loss : 0.652252\n",
      "step 34800 , training  accuracy 0.641453\n",
      "step 34800 , loss : 0.660526\n",
      "step 34900 , training  accuracy 0.658927\n",
      "step 34900 , loss : 0.659212\n",
      "step 35000 , training  accuracy 0.653799\n",
      "step 35000 , loss : 0.655989\n",
      "step 35100 , training  accuracy 0.654179\n",
      "step 35100 , loss : 0.65867\n",
      "step 35200 , training  accuracy 0.661396\n",
      "step 35200 , loss : 0.661092\n",
      "step 35300 , training  accuracy 0.662013\n",
      "step 35300 , loss : 0.654804\n",
      "step 35400 , training  accuracy 0.671273\n",
      "step 35400 , loss : 0.655134\n",
      "step 35500 , training  accuracy 0.663865\n",
      "step 35500 , loss : 0.654414\n",
      "step 35600 , training  accuracy 0.664672\n",
      "step 35600 , loss : 0.658694\n",
      "step 35700 , training  accuracy 0.673742\n",
      "step 35700 , loss : 0.654597\n",
      "step 35800 , training  accuracy 0.680105\n",
      "step 35800 , loss : 0.652402\n",
      "step 35900 , training  accuracy 0.690788\n",
      "step 35900 , loss : 0.654295\n",
      "step 36000 , training  accuracy 0.673124\n",
      "step 36000 , loss : 0.664166\n",
      "step 36100 , training  accuracy 0.683381\n",
      "step 36100 , loss : 0.650241\n",
      "step 36200 , training  accuracy 0.674786\n",
      "step 36200 , loss : 0.663101\n",
      "step 36300 , training  accuracy 0.652137\n",
      "step 36300 , loss : 0.673513\n",
      "step 36400 , training  accuracy 0.681387\n",
      "step 36400 , loss : 0.653493\n",
      "step 36500 , training  accuracy 0.690408\n",
      "step 36500 , loss : 0.654081\n",
      "step 36600 , training  accuracy 0.686705\n",
      "step 36600 , loss : 0.660562\n",
      "step 36700 , training  accuracy 0.686277\n",
      "step 36700 , loss : 0.65248\n",
      "step 36800 , training  accuracy 0.682811\n",
      "step 36800 , loss : 0.653696\n",
      "step 36900 , training  accuracy 0.703181\n",
      "step 36900 , loss : 0.65733\n",
      "step 37000 , training  accuracy 0.680152\n",
      "step 37000 , loss : 0.660086\n",
      "step 37100 , training  accuracy 0.681766\n",
      "step 37100 , loss : 0.658966\n",
      "step 37200 , training  accuracy 0.691643\n",
      "step 37200 , loss : 0.658614\n",
      "step 37300 , training  accuracy 0.702754\n",
      "step 37300 , loss : 0.658151\n",
      "step 37400 , training  accuracy 0.711823\n",
      "step 37400 , loss : 0.659125\n",
      "step 37500 , training  accuracy 0.708927\n",
      "step 37500 , loss : 0.65564\n",
      "step 37600 , training  accuracy 0.702944\n",
      "step 37600 , loss : 0.664584\n",
      "step 37700 , training  accuracy 0.690598\n",
      "step 37700 , loss : 0.664905\n",
      "step 37800 , training  accuracy 0.724169\n",
      "step 37800 , loss : 0.64818\n",
      "step 37900 , training  accuracy 0.718993\n",
      "step 37900 , loss : 0.652604\n",
      "step 38000 , training  accuracy 0.69207\n",
      "step 38000 , loss : 0.680682\n",
      "step 38100 , training  accuracy 0.724549\n",
      "step 38100 , loss : 0.649066\n",
      "step 38200 , training  accuracy 0.723742\n",
      "step 38200 , loss : 0.65642\n",
      "step 38300 , training  accuracy 0.721083\n",
      "step 38300 , loss : 0.659801\n",
      "step 38400 , training  accuracy 0.718566\n",
      "step 38400 , loss : 0.658704\n",
      "step 38500 , training  accuracy 0.717759\n",
      "step 38500 , loss : 0.654098\n",
      "step 38600 , training  accuracy 0.712821\n",
      "step 38600 , loss : 0.664123\n",
      "step 38700 , training  accuracy 0.724122\n",
      "step 38700 , loss : 0.656274\n",
      "step 38800 , training  accuracy 0.729297\n",
      "step 38800 , loss : 0.655401\n",
      "step 38900 , training  accuracy 0.712013\n",
      "step 38900 , loss : 0.663989\n",
      "step 39000 , training  accuracy 0.703371\n",
      "step 39000 , loss : 0.67601\n",
      "step 39100 , training  accuracy 0.720655\n",
      "step 39100 , loss : 0.657602\n",
      "step 39200 , training  accuracy 0.725973\n",
      "step 39200 , loss : 0.651478\n",
      "step 39300 , training  accuracy 0.713058\n",
      "step 39300 , loss : 0.660135\n",
      "step 39400 , training  accuracy 0.718186\n",
      "step 39400 , loss : 0.663355\n",
      "step 39500 , training  accuracy 0.710162\n",
      "step 39500 , loss : 0.662083\n",
      "step 39600 , training  accuracy 0.726638\n",
      "step 39600 , loss : 0.652884\n",
      "step 39700 , training  accuracy 0.72189\n",
      "step 39700 , loss : 0.667612\n",
      "step 39800 , training  accuracy 0.722697\n",
      "step 39800 , loss : 0.660184\n",
      "step 39900 , training  accuracy 0.724549\n",
      "step 39900 , loss : 0.657315\n",
      "step 40000 , training  accuracy 0.721463\n",
      "step 40000 , loss : 0.675296\n",
      "step 40100 , training  accuracy 0.707265\n",
      "step 40100 , loss : 0.668059\n",
      "step 40200 , training  accuracy 0.721463\n",
      "step 40200 , loss : 0.664537\n",
      "step 40300 , training  accuracy 0.725166\n",
      "step 40300 , loss : 0.658552\n",
      "step 40400 , training  accuracy 0.721463\n",
      "step 40400 , loss : 0.662161\n",
      "step 40500 , training  accuracy 0.727825\n",
      "step 40500 , loss : 0.653112\n",
      "step 40600 , training  accuracy 0.727018\n",
      "step 40600 , loss : 0.664285\n",
      "step 40700 , training  accuracy 0.706648\n",
      "step 40700 , loss : 0.668423\n",
      "step 40800 , training  accuracy 0.728253\n",
      "step 40800 , loss : 0.667901\n",
      "step 40900 , training  accuracy 0.733191\n",
      "step 40900 , loss : 0.664617\n",
      "step 41000 , training  accuracy 0.730484\n",
      "step 41000 , loss : 0.657585\n",
      "step 41100 , training  accuracy 0.727445\n",
      "step 41100 , loss : 0.663584\n",
      "step 41200 , training  accuracy 0.732764\n",
      "step 41200 , loss : 0.656539\n",
      "step 41300 , training  accuracy 0.712631\n",
      "step 41300 , loss : 0.673787\n",
      "step 41400 , training  accuracy 0.69943\n",
      "step 41400 , loss : 0.677875\n",
      "step 41500 , training  accuracy 0.707882\n",
      "step 41500 , loss : 0.672836\n",
      "step 41600 , training  accuracy 0.728443\n",
      "step 41600 , loss : 0.663993\n",
      "step 41700 , training  accuracy 0.707882\n",
      "step 41700 , loss : 0.67433\n",
      "step 41800 , training  accuracy 0.715907\n",
      "step 41800 , loss : 0.663266\n",
      "step 41900 , training  accuracy 0.724739\n",
      "step 41900 , loss : 0.666916\n",
      "step 42000 , training  accuracy 0.724312\n",
      "step 42000 , loss : 0.661284\n",
      "step 42100 , training  accuracy 0.723077\n",
      "step 42100 , loss : 0.662297\n",
      "step 42200 , training  accuracy 0.73585\n",
      "step 42200 , loss : 0.660553\n",
      "step 42300 , training  accuracy 0.714245\n",
      "step 42300 , loss : 0.669026\n",
      "step 42400 , training  accuracy 0.722887\n",
      "step 42400 , loss : 0.661779\n",
      "step 42500 , training  accuracy 0.728015\n",
      "step 42500 , loss : 0.664647\n",
      "step 42600 , training  accuracy 0.727825\n",
      "step 42600 , loss : 0.662799\n",
      "step 42700 , training  accuracy 0.713438\n",
      "step 42700 , loss : 0.667724\n",
      "step 42800 , training  accuracy 0.716904\n",
      "step 42800 , loss : 0.674014\n",
      "step 42900 , training  accuracy 0.717331\n",
      "step 42900 , loss : 0.666656\n",
      "step 43000 , training  accuracy 0.717521\n",
      "step 43000 , loss : 0.674241\n",
      "step 43100 , training  accuracy 0.709069\n",
      "step 43100 , loss : 0.678555\n",
      "step 43200 , training  accuracy 0.725119\n",
      "step 43200 , loss : 0.664617\n",
      "step 43300 , training  accuracy 0.723504\n",
      "step 43300 , loss : 0.666378\n",
      "step 43400 , training  accuracy 0.714435\n",
      "step 43400 , loss : 0.673105\n",
      "step 43500 , training  accuracy 0.707882\n",
      "step 43500 , loss : 0.672861\n",
      "step 43600 , training  accuracy 0.712631\n",
      "step 43600 , loss : 0.681637\n",
      "step 43700 , training  accuracy 0.720608\n",
      "step 43700 , loss : 0.667172\n",
      "step 43800 , training  accuracy 0.725356\n",
      "step 43800 , loss : 0.665337\n",
      "step 43900 , training  accuracy 0.723504\n",
      "step 43900 , loss : 0.663793\n",
      "step 44000 , training  accuracy 0.720845\n",
      "step 44000 , loss : 0.665187\n",
      "step 44100 , training  accuracy 0.734188\n",
      "step 44100 , loss : 0.660142\n",
      "step 44200 , training  accuracy 0.727825\n",
      "step 44200 , loss : 0.659899\n",
      "step 44300 , training  accuracy 0.718139\n",
      "step 44300 , loss : 0.66415\n",
      "step 44400 , training  accuracy 0.726781\n",
      "step 44400 , loss : 0.665015\n",
      "step 44500 , training  accuracy 0.744065\n",
      "step 44500 , loss : 0.656513\n",
      "step 44600 , training  accuracy 0.721225\n",
      "step 44600 , loss : 0.673459\n",
      "step 44700 , training  accuracy 0.705413\n",
      "step 44700 , loss : 0.680472\n",
      "step 44800 , training  accuracy 0.709734\n",
      "step 44800 , loss : 0.688744\n",
      "step 44900 , training  accuracy 0.716762\n",
      "step 44900 , loss : 0.67635\n",
      "step 45000 , training  accuracy 0.713438\n",
      "step 45000 , loss : 0.670469\n",
      "step 45100 , training  accuracy 0.719611\n",
      "step 45100 , loss : 0.677564\n",
      "step 45200 , training  accuracy 0.728633\n",
      "step 45200 , loss : 0.662047\n",
      "step 45300 , training  accuracy 0.721463\n",
      "step 45300 , loss : 0.666737\n",
      "step 45400 , training  accuracy 0.709972\n",
      "step 45400 , loss : 0.672503\n",
      "step 45500 , training  accuracy 0.733381\n",
      "step 45500 , loss : 0.655226\n",
      "step 45600 , training  accuracy 0.722317\n",
      "step 45600 , loss : 0.671197\n",
      "step 45700 , training  accuracy 0.72887\n",
      "step 45700 , loss : 0.658462\n",
      "step 45800 , training  accuracy 0.720465\n",
      "step 45800 , loss : 0.661765\n",
      "step 45900 , training  accuracy 0.722317\n",
      "step 45900 , loss : 0.666348\n",
      "step 46000 , training  accuracy 0.72208\n",
      "step 46000 , loss : 0.678635\n",
      "step 46100 , training  accuracy 0.716952\n",
      "step 46100 , loss : 0.667161\n",
      "step 46200 , training  accuracy 0.712821\n",
      "step 46200 , loss : 0.669235\n",
      "step 46300 , training  accuracy 0.720418\n",
      "step 46300 , loss : 0.673544\n",
      "step 46400 , training  accuracy 0.72208\n",
      "step 46400 , loss : 0.665588\n",
      "step 46500 , training  accuracy 0.730294\n",
      "step 46500 , loss : 0.663054\n",
      "step 46600 , training  accuracy 0.717331\n",
      "step 46600 , loss : 0.667218\n",
      "step 46700 , training  accuracy 0.726591\n",
      "step 46700 , loss : 0.670662\n",
      "step 46800 , training  accuracy 0.724739\n",
      "step 46800 , loss : 0.665124\n",
      "step 46900 , training  accuracy 0.734236\n",
      "step 46900 , loss : 0.662188\n",
      "step 47000 , training  accuracy 0.718376\n",
      "step 47000 , loss : 0.673131\n",
      "step 47100 , training  accuracy 0.724976\n",
      "step 47100 , loss : 0.659064\n",
      "step 47200 , training  accuracy 0.738509\n",
      "step 47200 , loss : 0.655549\n",
      "step 47300 , training  accuracy 0.734615\n",
      "step 47300 , loss : 0.658455\n",
      "step 47400 , training  accuracy 0.69226\n",
      "step 47400 , loss : 0.684277\n",
      "step 47500 , training  accuracy 0.718803\n",
      "step 47500 , loss : 0.671437\n",
      "step 47600 , training  accuracy 0.724359\n",
      "step 47600 , loss : 0.672978\n",
      "step 47700 , training  accuracy 0.726401\n",
      "step 47700 , loss : 0.66304\n",
      "step 47800 , training  accuracy 0.726828\n",
      "step 47800 , loss : 0.664741\n",
      "step 47900 , training  accuracy 0.729297\n",
      "step 47900 , loss : 0.664927\n",
      "step 48000 , training  accuracy 0.729915\n",
      "step 48000 , loss : 0.665565\n",
      "step 48100 , training  accuracy 0.735043\n",
      "step 48100 , loss : 0.657527\n",
      "step 48200 , training  accuracy 0.721273\n",
      "step 48200 , loss : 0.669467\n",
      "step 48300 , training  accuracy 0.717142\n",
      "step 48300 , loss : 0.67053\n",
      "step 48400 , training  accuracy 0.727445\n",
      "step 48400 , loss : 0.665656\n",
      "step 48500 , training  accuracy 0.732146\n",
      "step 48500 , loss : 0.664725\n",
      "step 48600 , training  accuracy 0.719421\n",
      "step 48600 , loss : 0.66956\n",
      "step 48700 , training  accuracy 0.707265\n",
      "step 48700 , loss : 0.682942\n",
      "step 48800 , training  accuracy 0.731766\n",
      "step 48800 , loss : 0.667892\n",
      "step 48900 , training  accuracy 0.733001\n",
      "step 48900 , loss : 0.663659\n",
      "step 49000 , training  accuracy 0.719611\n",
      "step 49000 , loss : 0.675924\n",
      "step 49100 , training  accuracy 0.721652\n",
      "step 49100 , loss : 0.670657\n",
      "step 49200 , training  accuracy 0.717759\n",
      "step 49200 , loss : 0.677533\n",
      "step 49300 , training  accuracy 0.719611\n",
      "step 49300 , loss : 0.676004\n",
      "step 49400 , training  accuracy 0.711586\n",
      "step 49400 , loss : 0.675981\n",
      "step 49500 , training  accuracy 0.693922\n",
      "step 49500 , loss : 0.690625\n",
      "step 49600 , training  accuracy 0.693305\n",
      "step 49600 , loss : 0.688384\n",
      "step 49700 , training  accuracy 0.705841\n",
      "step 49700 , loss : 0.678264\n",
      "step 49800 , training  accuracy 0.707075\n",
      "step 49800 , loss : 0.677011\n",
      "step 49900 , training  accuracy 0.730912\n",
      "step 49900 , loss : 0.660273\n",
      "step 50000 , training  accuracy 0.723742\n",
      "step 50000 , loss : 0.668157\n",
      "step 50100 , training  accuracy 0.720465\n",
      "step 50100 , loss : 0.669554\n",
      "step 50200 , training  accuracy 0.727445\n",
      "step 50200 , loss : 0.661086\n",
      "step 50300 , training  accuracy 0.709972\n",
      "step 50300 , loss : 0.669256\n",
      "step 50400 , training  accuracy 0.725594\n",
      "step 50400 , loss : 0.665546\n",
      "step 50500 , training  accuracy 0.720655\n",
      "step 50500 , loss : 0.671171\n",
      "step 50600 , training  accuracy 0.721083\n",
      "step 50600 , loss : 0.667449\n",
      "step 50700 , training  accuracy 0.730342\n",
      "step 50700 , loss : 0.6642\n",
      "step 50800 , training  accuracy 0.736705\n",
      "step 50800 , loss : 0.666394\n",
      "step 50900 , training  accuracy 0.729297\n",
      "step 50900 , loss : 0.663503\n",
      "step 51000 , training  accuracy 0.725594\n",
      "step 51000 , loss : 0.662093\n",
      "step 51100 , training  accuracy 0.715527\n",
      "step 51100 , loss : 0.663223\n",
      "step 51200 , training  accuracy 0.713675\n",
      "step 51200 , loss : 0.671605\n",
      "step 51300 , training  accuracy 0.703799\n",
      "step 51300 , loss : 0.67864\n",
      "step 51400 , training  accuracy 0.718376\n",
      "step 51400 , loss : 0.678572\n",
      "step 51500 , training  accuracy 0.709354\n",
      "step 51500 , loss : 0.672643\n",
      "step 51600 , training  accuracy 0.703561\n",
      "step 51600 , loss : 0.679327\n",
      "step 51700 , training  accuracy 0.711966\n",
      "step 51700 , loss : 0.673683\n",
      "step 51800 , training  accuracy 0.719801\n",
      "step 51800 , loss : 0.673455\n",
      "step 51900 , training  accuracy 0.70603\n",
      "step 51900 , loss : 0.668366\n",
      "step 52000 , training  accuracy 0.712868\n",
      "step 52000 , loss : 0.678994\n",
      "step 52100 , training  accuracy 0.721842\n",
      "step 52100 , loss : 0.674786\n",
      "step 52200 , training  accuracy 0.726971\n",
      "step 52200 , loss : 0.664455\n",
      "step 52300 , training  accuracy 0.723932\n",
      "step 52300 , loss : 0.668297\n",
      "step 52400 , training  accuracy 0.72189\n",
      "step 52400 , loss : 0.666559\n",
      "step 52500 , training  accuracy 0.726401\n",
      "step 52500 , loss : 0.665231\n",
      "step 52600 , training  accuracy 0.720418\n",
      "step 52600 , loss : 0.666615\n",
      "step 52700 , training  accuracy 0.732764\n",
      "step 52700 , loss : 0.663811\n",
      "step 52800 , training  accuracy 0.727018\n",
      "step 52800 , loss : 0.659859\n",
      "step 52900 , training  accuracy 0.727445\n",
      "step 52900 , loss : 0.664836\n",
      "step 53000 , training  accuracy 0.736277\n",
      "step 53000 , loss : 0.654912\n",
      "step 53100 , training  accuracy 0.743685\n",
      "step 53100 , loss : 0.653269\n",
      "step 53200 , training  accuracy 0.741833\n",
      "step 53200 , loss : 0.653878\n",
      "step 53300 , training  accuracy 0.738936\n",
      "step 53300 , loss : 0.648357\n",
      "step 53400 , training  accuracy 0.730722\n",
      "step 53400 , loss : 0.653249\n",
      "step 53500 , training  accuracy 0.73547\n",
      "step 53500 , loss : 0.6526\n",
      "step 53600 , training  accuracy 0.743875\n",
      "step 53600 , loss : 0.652422\n",
      "step 53700 , training  accuracy 0.733808\n",
      "step 53700 , loss : 0.649885\n",
      "step 53800 , training  accuracy 0.731339\n",
      "step 53800 , loss : 0.652895\n",
      "step 53900 , training  accuracy 0.734426\n",
      "step 53900 , loss : 0.646098\n",
      "step 54000 , training  accuracy 0.736895\n",
      "step 54000 , loss : 0.651231\n",
      "step 54100 , training  accuracy 0.731339\n",
      "step 54100 , loss : 0.646591\n",
      "step 54200 , training  accuracy 0.733191\n",
      "step 54200 , loss : 0.645955\n",
      "step 54300 , training  accuracy 0.727635\n",
      "step 54300 , loss : 0.655174\n",
      "step 54400 , training  accuracy 0.728253\n",
      "step 54400 , loss : 0.650557\n",
      "step 54500 , training  accuracy 0.729677\n",
      "step 54500 , loss : 0.654863\n",
      "step 54600 , training  accuracy 0.733808\n",
      "step 54600 , loss : 0.649182\n",
      "step 54700 , training  accuracy 0.733381\n",
      "step 54700 , loss : 0.658879\n",
      "step 54800 , training  accuracy 0.70152\n",
      "step 54800 , loss : 0.681149\n",
      "step 54900 , training  accuracy 0.687939\n",
      "step 54900 , loss : 0.684638\n",
      "step 55000 , training  accuracy 0.700665\n",
      "step 55000 , loss : 0.689828\n",
      "step 55100 , training  accuracy 0.695774\n",
      "step 55100 , loss : 0.688329\n",
      "step 55200 , training  accuracy 0.709164\n",
      "step 55200 , loss : 0.683809\n",
      "step 55300 , training  accuracy 0.707502\n",
      "step 55300 , loss : 0.677822\n",
      "step 55400 , training  accuracy 0.712013\n",
      "step 55400 , loss : 0.674148\n",
      "step 55500 , training  accuracy 0.721225\n",
      "step 55500 , loss : 0.67149\n",
      "step 55600 , training  accuracy 0.72208\n",
      "step 55600 , loss : 0.677947\n",
      "step 55700 , training  accuracy 0.721273\n",
      "step 55700 , loss : 0.669836\n",
      "step 55800 , training  accuracy 0.720465\n",
      "step 55800 , loss : 0.669117\n",
      "step 55900 , training  accuracy 0.722317\n",
      "step 55900 , loss : 0.665772\n",
      "step 56000 , training  accuracy 0.721463\n",
      "step 56000 , loss : 0.660766\n",
      "step 56100 , training  accuracy 0.72227\n",
      "step 56100 , loss : 0.661609\n",
      "step 56200 , training  accuracy 0.737322\n",
      "step 56200 , loss : 0.653594\n",
      "step 56300 , training  accuracy 0.722507\n",
      "step 56300 , loss : 0.659842\n",
      "step 56400 , training  accuracy 0.724169\n",
      "step 56400 , loss : 0.658972\n",
      "step 56500 , training  accuracy 0.725784\n",
      "step 56500 , loss : 0.655772\n",
      "step 56600 , training  accuracy 0.721273\n",
      "step 56600 , loss : 0.652742\n",
      "step 56700 , training  accuracy 0.716952\n",
      "step 56700 , loss : 0.658113\n",
      "step 56800 , training  accuracy 0.726211\n",
      "step 56800 , loss : 0.654802\n",
      "step 56900 , training  accuracy 0.714103\n",
      "step 56900 , loss : 0.662657\n",
      "step 57000 , training  accuracy 0.720465\n",
      "step 57000 , loss : 0.654968\n",
      "step 57100 , training  accuracy 0.720655\n",
      "step 57100 , loss : 0.658568\n",
      "step 57200 , training  accuracy 0.717379\n",
      "step 57200 , loss : 0.652608\n",
      "step 57300 , training  accuracy 0.729915\n",
      "step 57300 , loss : 0.652285\n",
      "step 57400 , training  accuracy 0.738984\n",
      "step 57400 , loss : 0.646957\n",
      "step 57500 , training  accuracy 0.726211\n",
      "step 57500 , loss : 0.646505\n",
      "step 57600 , training  accuracy 0.721083\n",
      "step 57600 , loss : 0.648757\n",
      "step 57700 , training  accuracy 0.729725\n",
      "step 57700 , loss : 0.645721\n",
      "step 57800 , training  accuracy 0.726638\n",
      "step 57800 , loss : 0.647524\n",
      "step 57900 , training  accuracy 0.7217\n",
      "step 57900 , loss : 0.656287\n",
      "step 58000 , training  accuracy 0.725214\n",
      "step 58000 , loss : 0.646906\n",
      "step 58100 , training  accuracy 0.726448\n",
      "step 58100 , loss : 0.652233\n",
      "step 58200 , training  accuracy 0.732764\n",
      "step 58200 , loss : 0.651765\n",
      "step 58300 , training  accuracy 0.67868\n",
      "step 58300 , loss : 0.683772\n",
      "step 58400 , training  accuracy 0.705413\n",
      "step 58400 , loss : 0.668858\n",
      "step 58500 , training  accuracy 0.70603\n",
      "step 58500 , loss : 0.681748\n",
      "step 58600 , training  accuracy 0.718614\n",
      "step 58600 , loss : 0.674519\n",
      "step 58700 , training  accuracy 0.727873\n",
      "step 58700 , loss : 0.667945\n",
      "step 58800 , training  accuracy 0.7151\n",
      "step 58800 , loss : 0.688575\n",
      "step 58900 , training  accuracy 0.716334\n",
      "step 58900 , loss : 0.670947\n",
      "step 59000 , training  accuracy 0.737322\n",
      "step 59000 , loss : 0.661926\n",
      "step 59100 , training  accuracy 0.725404\n",
      "step 59100 , loss : 0.664978\n",
      "step 59200 , training  accuracy 0.72849\n",
      "step 59200 , loss : 0.659206\n",
      "step 59300 , training  accuracy 0.743685\n",
      "step 59300 , loss : 0.656282\n",
      "step 59400 , training  accuracy 0.736895\n",
      "step 59400 , loss : 0.653395\n",
      "step 59500 , training  accuracy 0.726828\n",
      "step 59500 , loss : 0.654516\n",
      "step 59600 , training  accuracy 0.726828\n",
      "step 59600 , loss : 0.66015\n",
      "step 59700 , training  accuracy 0.727635\n",
      "step 59700 , loss : 0.655711\n",
      "step 59800 , training  accuracy 0.718993\n",
      "step 59800 , loss : 0.663589\n",
      "step 59900 , training  accuracy 0.724549\n",
      "step 59900 , loss : 0.659497\n",
      "step 60000 , training  accuracy 0.740408\n",
      "step 60000 , loss : 0.656076\n",
      "step 60100 , training  accuracy 0.739981\n",
      "step 60100 , loss : 0.651111\n",
      "step 60200 , training  accuracy 0.72868\n",
      "step 60200 , loss : 0.657105\n",
      "step 60300 , training  accuracy 0.73547\n",
      "step 60300 , loss : 0.651085\n",
      "step 60400 , training  accuracy 0.739364\n",
      "step 60400 , loss : 0.652749\n",
      "step 60500 , training  accuracy 0.73528\n",
      "step 60500 , loss : 0.656136\n",
      "step 60600 , training  accuracy 0.725594\n",
      "step 60600 , loss : 0.660287\n",
      "step 60700 , training  accuracy 0.725594\n",
      "step 60700 , loss : 0.661277\n",
      "step 60800 , training  accuracy 0.720655\n",
      "step 60800 , loss : 0.676624\n",
      "step 60900 , training  accuracy 0.7151\n",
      "step 60900 , loss : 0.670226\n",
      "step 61000 , training  accuracy 0.709734\n",
      "step 61000 , loss : 0.680861\n",
      "step 61100 , training  accuracy 0.698006\n",
      "step 61100 , loss : 0.683267\n",
      "step 61200 , training  accuracy 0.703989\n",
      "step 61200 , loss : 0.680929\n",
      "step 61300 , training  accuracy 0.713438\n",
      "step 61300 , loss : 0.670634\n",
      "step 61400 , training  accuracy 0.725594\n",
      "step 61400 , loss : 0.665519\n",
      "step 61500 , training  accuracy 0.732384\n",
      "step 61500 , loss : 0.665754\n",
      "step 61600 , training  accuracy 0.727635\n",
      "step 61600 , loss : 0.668358\n",
      "step 61700 , training  accuracy 0.729297\n",
      "step 61700 , loss : 0.659406\n",
      "step 61800 , training  accuracy 0.731339\n",
      "step 61800 , loss : 0.666413\n",
      "step 61900 , training  accuracy 0.731956\n",
      "step 61900 , loss : 0.653187\n",
      "step 62000 , training  accuracy 0.734853\n",
      "step 62000 , loss : 0.658794\n",
      "step 62100 , training  accuracy 0.735233\n",
      "step 62100 , loss : 0.655423\n",
      "step 62200 , training  accuracy 0.730532\n",
      "step 62200 , loss : 0.656948\n",
      "step 62300 , training  accuracy 0.733381\n",
      "step 62300 , loss : 0.665409\n",
      "step 62400 , training  accuracy 0.733381\n",
      "step 62400 , loss : 0.65828\n",
      "step 62500 , training  accuracy 0.740171\n",
      "step 62500 , loss : 0.659187\n",
      "step 62600 , training  accuracy 0.731339\n",
      "step 62600 , loss : 0.660967\n",
      "step 62700 , training  accuracy 0.733381\n",
      "step 62700 , loss : 0.661897\n",
      "step 62800 , training  accuracy 0.728443\n",
      "step 62800 , loss : 0.663079\n",
      "step 62900 , training  accuracy 0.740788\n",
      "step 62900 , loss : 0.666092\n",
      "step 63000 , training  accuracy 0.710969\n",
      "step 63000 , loss : 0.672038\n",
      "step 63100 , training  accuracy 0.719421\n",
      "step 63100 , loss : 0.671339\n",
      "step 63200 , training  accuracy 0.730105\n",
      "step 63200 , loss : 0.663187\n",
      "step 63300 , training  accuracy 0.730105\n",
      "step 63300 , loss : 0.663464\n",
      "step 63400 , training  accuracy 0.724739\n",
      "step 63400 , loss : 0.671471\n",
      "step 63500 , training  accuracy 0.705413\n",
      "step 63500 , loss : 0.683341\n",
      "step 63600 , training  accuracy 0.718186\n",
      "step 63600 , loss : 0.67276\n",
      "step 63700 , training  accuracy 0.724549\n",
      "step 63700 , loss : 0.670776\n",
      "step 63800 , training  accuracy 0.707692\n",
      "step 63800 , loss : 0.678665\n",
      "step 63900 , training  accuracy 0.737749\n",
      "step 63900 , loss : 0.664337\n",
      "step 64000 , training  accuracy 0.715717\n",
      "step 64000 , loss : 0.676853\n",
      "step 64100 , training  accuracy 0.721273\n",
      "step 64100 , loss : 0.661898\n",
      "step 64200 , training  accuracy 0.725594\n",
      "step 64200 , loss : 0.664476\n",
      "step 64300 , training  accuracy 0.732764\n",
      "step 64300 , loss : 0.661461\n",
      "step 64400 , training  accuracy 0.721463\n",
      "step 64400 , loss : 0.666975\n",
      "step 64500 , training  accuracy 0.730294\n",
      "step 64500 , loss : 0.660777\n",
      "step 64600 , training  accuracy 0.736277\n",
      "step 64600 , loss : 0.657834\n",
      "step 64700 , training  accuracy 0.73547\n",
      "step 64700 , loss : 0.662051\n",
      "step 64800 , training  accuracy 0.731956\n",
      "step 64800 , loss : 0.665803\n",
      "step 64900 , training  accuracy 0.721273\n",
      "step 64900 , loss : 0.666641\n",
      "step 65000 , training  accuracy 0.726211\n",
      "step 65000 , loss : 0.667522\n",
      "step 65100 , training  accuracy 0.729297\n",
      "step 65100 , loss : 0.666092\n",
      "step 65200 , training  accuracy 0.725166\n",
      "step 65200 , loss : 0.666734\n",
      "step 65300 , training  accuracy 0.723314\n",
      "step 65300 , loss : 0.670461\n",
      "step 65400 , training  accuracy 0.740598\n",
      "step 65400 , loss : 0.660333\n",
      "step 65500 , training  accuracy 0.714245\n",
      "step 65500 , loss : 0.675558\n",
      "step 65600 , training  accuracy 0.724359\n",
      "step 65600 , loss : 0.664454\n",
      "step 65700 , training  accuracy 0.70622\n",
      "step 65700 , loss : 0.679877\n",
      "step 65800 , training  accuracy 0.724739\n",
      "step 65800 , loss : 0.676009\n",
      "step 65900 , training  accuracy 0.714055\n",
      "step 65900 , loss : 0.678648\n",
      "step 66000 , training  accuracy 0.719611\n",
      "step 66000 , loss : 0.672219\n",
      "step 66100 , training  accuracy 0.717379\n",
      "step 66100 , loss : 0.675726\n",
      "step 66200 , training  accuracy 0.723124\n",
      "step 66200 , loss : 0.672117\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "#sm_conv= tf.nn.softmax(y_conv)\n",
    "    #cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "    start_time = time.time()\n",
    "\n",
    "    regular=0.01*(tf.reduce_sum(tf.square(y_conv)))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( y_conv, y_))\n",
    "with tf.device('/gpu:3'):\n",
    "    cost = cost+regular\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cost) #1e-4\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_conv,1) ,tf.argmax(y_,1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\")) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(iterate):\n",
    "    \n",
    "    batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)\n",
    "   # batch_val_xs  , batch_val_ys = next_batch(20 , val_img , val_lab)\n",
    "    if i%100 ==0: # in here add to validation \n",
    "        try:\n",
    "            train_accuracy = sess.run( accuracy , feed_dict={x:val_img , y_:val_lab , keep_prob: 1.0})        \n",
    "            loss = sess.run(cost , feed_dict = {x:val_img , y_: val_lab , keep_prob: 1.0})\n",
    "\n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,loss))\n",
    "            str_ = 'step :'+str(i)+'loss :'+str(loss) +'training accuracy :'+str(train_accuracy)+'\\n'\n",
    "            f.write(str_)\n",
    "    \n",
    "        except :\n",
    "            list_acc=[]\n",
    "            list_loss=[]\n",
    "            n_divide=len(val_img)/batch_size\n",
    "            for j in range(n_divide):\n",
    "                \n",
    "                # j*batch_size :(j+1)*batch_size\n",
    "                train_accuracy,loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :(j+1)*batch_size] , y_:val_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "                list_acc.append(float(train_accuracy))\n",
    "                list_loss.append(float(loss))\n",
    "            train_accuracy , loss=sess.run([accuracy,cost] , feed_dict={x:val_img[(j+1)*batch_size : ] , y_:val_lab[(j+1)*(batch_size) : ] , keep_prob : 1.0})\n",
    "            #right above code have to modify\n",
    "            \n",
    "            list_acc.append(train_accuracy)\n",
    "            list_loss.append(loss)\n",
    "            list_acc=np.asarray(list_acc)\n",
    "            list_loss= np.asarray(list_loss)\n",
    "            \n",
    "            train_accuracy=np.mean(list_acc)\n",
    "            loss = np.mean(list_loss)\n",
    "            \n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,loss))\n",
    "            str_ = 'step :'+str(i)+'loss :'+str(loss) +'training accuracy :'+str(train_accuracy)+'\\n'\n",
    "            f.write(str_)\n",
    "    \n",
    "    \n",
    "    sess.run(train_step ,feed_dict={x:batch_xs , y_:batch_ys , keep_prob : 0.7})\n",
    "print(\"test accuracy %g\" %sess.run(accuracy , \n",
    "                                   feed_dict={x:test_img , y_:test_lab, keep_prob : 1.0 }))\n",
    "\n",
    "print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "str_='test accuracy'+str(accuracy)\n",
    "f.write(str_)\n",
    "#추가 한 부분 \n",
    "test_pred , test_acc = sess.run([y_conv , accuracy] , feed_dict={x:test_img , y_:test_lab,keep_prob : 1.0})\n",
    "f.write(str(test_acc))\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def file2Graph(file_path , save_path , graph_size):\n",
    "    assert type(graph_size) == tuple \n",
    "    \n",
    "    step=[]\n",
    "    loss=[]\n",
    "    acc=[]\n",
    "    test_acc=0.0\n",
    "    fo=open(file_path)\n",
    "    lines=fo.readlines()\n",
    "    for line in lines:\n",
    "        list_splited_line=line.split()\n",
    "        if 'step' in list_splited_line:\n",
    "            print list_splited_line[2]\n",
    "            step.append(float(list_splited_line[2]))\n",
    "            \n",
    "        if 'loss' in line:\n",
    "            loss.append(float(list_splited_line[5]))\n",
    "        if 'training accuracy' in line:\n",
    "            acc.append(float(list_splited_line[9]))\n",
    "        if 'test accuracy' in line:\n",
    "            test_acc=float(list_splited_line[3])\n",
    "            \n",
    "    #need 4 4graph \n",
    "    fig1 = plt.figure(figsize =graph_size)\n",
    "    save_name='x: step , y : loss'\n",
    "    ax=fig1.add_subplot(1,1,1)\n",
    "    plt.plot(step , loss)\n",
    "    plt.savefig(save_path+save_name)\n",
    "    \n",
    "    fig2 = plt.figure(figsize =graph_size)\n",
    "    ax1=fig2.add_subplot(1,1,1)\n",
    "    save_name='x: step , y : acc'\n",
    "    plt.plot(step , acc)\n",
    "    plt.savefig(save_path+save_name)\n",
    "    \n",
    "    fig3 = plt.figure(figsize =graph_size)\n",
    "    ax2=fig3.add_subplot(1,1,1)\n",
    "    \n",
    "    save_name='x: step , y(blue) : acc , y2(green) : loss'\n",
    "    plt.plot(step , acc ,'b')\n",
    "    plt.plot(step , loss ,'g')\n",
    "    plt.savefig(save_path+save_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Image\n",
    "result_list=list(result_np)\n",
    "lab_list=list(lab_np)\n",
    "pred_list=list(pred_np)\n",
    "#\n",
    "#def savepic(save_path,extension,img_source,lab_source, pred_list , result_list=None, img_row , img_col,color_ch=1 ):\n",
    "\n",
    "import Utility_\n",
    "save_path ='/home/ubuntu/Desktop/delete_'\n",
    "extension='.jpg'\n",
    "img_source = test_img\n",
    "print np.shape(img_source)\n",
    "if 'numpy'  in str(type(test_img)):\n",
    "    print 'numpy'\n",
    "print type(img_source)\n",
    "img=Utility_.savepic(save_path , extension , img_source , lab_list , pred_list , result_list, img_row=64 ,\\\n",
    "                 img_col=64 , color_ch =1)\n",
    "img=img.reshape(img_row , img_col)\n",
    "print np.shape(img)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
